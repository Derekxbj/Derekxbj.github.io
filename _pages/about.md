---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am researcher 



# üî• News
- *2023.11*: &nbsp;üéâüéâ I have pased my PhD dissertation defense!


# üìù Projects and Publications 

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2016</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

**Kaiming He**, Xiangyu Zhang, Shaoqing Ren, Jian Sun

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div> -->


## Human-Robot Interaction

<div class='paper-box'>
<div class='paper-box-image'><div><div class="badge">CAREER</div><img src='images/nsf_project.png' alt="sym" width="120%"></div></div>


<div class='paper-box-text' markdown="1">

*2019.03 - now*: &nbsp; **<font color='red'> As a core member, </font>** I participated in the project of [CAREER: Social Intelligence with Contextual Ambidexterity for Long-Term Human-Robot Interaction and Intervention (LT-HRI2)](http://www.chunghyukpark.com/robots-for-autistic-adolescents.html). 

- The goal of this project is to understand the fundamental principles of human interactions and behaviors and translate these mechanisms into computational modeling and algorithms for a novel assistive robotic framework. Toward this goal, we developed a socially assistive robotic framework with contextual ambidexterity that is perceptive of personal socio-emotional states, capable of learning social skills, emotionally interactive, and gender-smart for long-term human-robot interaction and intervention. Several publications related to this project:

</div>

<!-- Publications -->
<div class='publications' markdown="1">


- [" Can You Guess My Moves? Playing Charades with a Humanoid Robot Employing Mutual Learning with Emotional Intelligence](https://dl.acm.org/doi/abs/10.1145/3568294.3580170), **Baijun Xie**, Chung Hyuk Park, *Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction*

- [Robust Multimodal Emotion Recognition from Conversation with Transformer-Based Crossmodality Fusion](https://www.mdpi.com/1424-8220/21/14/4913), **Baijun Xie**, Mariia Sidulova, Chung Hyuk Park, *Sensors*, 2021

- [Empathetic Robot With Transformer-Based Dialogue Agent](https://ieeexplore.ieee.org/abstract/document/9494669), **Baijun Xie**, Chung Hyuk Park, *2021 18th International Conference on Ubiquitous Robots (UR)*

- [Dance with a Robot: Encoder-Decoder Neural Network for Music-Dance Learning](https://dl.acm.org/doi/abs/10.1145/3371382.3378372), **Baijun Xie**, Chung Hyuk Park, *Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction*

- [Musical emotion recognition with spectral feature extraction based on a sinusoidal model with model-based and deep-learning approaches](https://www.mdpi.com/2076-3417/10/3/902), **Baijun Xie**, Jonathan C Kim, Chung Hyuk Park, *Applied Sciences*, 2020

</div>
</div>


## Social Intelligence

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/siq_ex.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Multi-Modal Correlated Network with Emotional Reasoning Knowledge for Social Intelligence Question-Answering](https://openaccess.thecvf.com/content/ICCV2023W/ASI/papers/Xie_Multi-Modal_Correlated_Network_with_Emotional_Reasoning_Knowledge_for_Social_Intelligence_ICCVW_2023_paper.pdf)

**Baijun Xie**, Chung Hyuk Park, *Proceedings of the IEEE/CVF International Conference on Computer Vision* [\[Code\]](https://github.com/Derekxbj/Social-IQ-2.0-Multimodal-with-Emotional-Cues)

- We participate the [Social IQ 2.0 Challenge](https://cmu-multicomp-lab.github.io/social-iq-2.0/), which is designed to benchmark recent AI technologies' skills to reason about social interactions, which is referred as Artificial Social Intelligence.
- We developed a framework named Multi-Modal Temporal Correlated Network with Emotional Social Cues (MMTC-ESC). MMTC-ESC exhibits an attention-based mechanism to model cross-modal correlations and utilizes contrastive learning for reasoning the emotional social cues.


</div>
</div>




## Medical Imaging

<div class='paper-box'>
<div class='paper-box-image'><div><div class="badge">NERVE</div><img src='images/nerve_project.jpg' alt="sym" width="120%"></div></div>


<div class='paper-box-text' markdown="1">

*2019.03 - now*: &nbsp; **<font color='red'> As a core member, </font>** I participated in the project of [Real-Time Nerve Tissue Detection and Visualization](http://www.chunghyukpark.com/real-time-nerve-tissue-detection-and-visualization.html). 

- We proposed the usage of birefringence images with deep learning inference to help with the detection of nerves. The study findings showed that leveraging birefringence images outperforms its RGB counterpart for nerve detection and segmentation. Additionally, introducing the crossattention module on a single modality network improved the detection and segmentation of nerve structures on the birefringence images.

</div>

<!-- Publications -->
<div class='publications' markdown="1">

- [DXM‚ÄêTransFuse U-net: Dual cross-modal transformer fusion U-net for automated nerve identification](https://www.sciencedirect.com/science/article/pii/S0895611122000635), **Baijun Xie**, Gary Milam, Bo Ning, Jaepyeong Cha, Chung Hyuk Park, *Computerized Medical Imaging and Graphics*, 2022

- ``Patent`` Real Time Automated Nerve Identification System.‚Äù International Publication No. WO2023/183930, Gary Milam, **Baijun Xie**, Chung Hyuk Park

</div>
</div>



<!-- - [Multi-Modal Correlated Network with Emotional Reasoning Knowledge for Social Intelligence Question-Answering](https://openaccess.thecvf.com/content/ICCV2023W/ASI/html/Xie_Multi-Modal_Correlated_Network_with_Emotional_Reasoning_Knowledge_for_Social_Intelligence_ICCVW_2023_paper.html), Baijun Xie, Chung Hyuk Park, *Proceedings of the IEEE/CVF International Conference on Computer Vision* -->


## Others

- [A MultiModal Social Robot Toward Personalized Emotion Interaction](https://arxiv.org/pdf/2110.05186.pdf), **Baijun Xie**, Chung Hyuk Park, *arXiv preprint*, Submitted to AAAI 2023 Fall Symposium

- [Trainable Quaternion Extended Kalman Filter with Multi-Head Attention for Dead Reckoning in Autonomous Ground Vehicles](https://www.mdpi.com/1424-8220/22/20/7701), Gary Milam, **Baijun Xie**, Runnan Liu, Xiaoheng Zhu, Juyoun Park, Gonwoo Kim, Chung Hyuk Park, *Sensors*, 2022







# üéñ Honors and Awards
- *2023.01* 2022 Collins Distinguished Doctoral Fellowship at the GWU. 
- *2022.04* 2022 GW Technology Commercialization Innovation Competition ([Audience‚Äôs choice posters Prize](https://commercialization.gwu.edu/tco-innovation-competition)). 
- *2022.01* 2021 Collins Distinguished Doctoral Fellowship at the GWU. 
- *2016.05* 2016 A Hundred Excellent Final Year Theses Prize in Shenzhen University . 
- *2015.09* 2015 Shenzhen University Challenge Cup University Student Venture Contest (2nd Prize). 
- *2014.09* Laboratory Open Fund Project of Shenzhen University (3rd Prize). 


# üìñ Educations
- *2018.09 - 2023.11 (now)*, Doctor of Philosophy, The George Washington University, Washington DC, U.S.A.
- *2016.09 - 2018.05*, Master of Science, The George Washington University, Washington DC, U.S.A. 
- *2012.09 - 2016.06*, Bachelor of Engineering, Shenzhen University, Guandong, China. 

# üí¨ Invited Talks
## Oral
- *2023.10*, 2023 IEEE/CVF International Conference on Computer Vision (ICCV) Workshops at Paris, France
- *2022.11*, 2022 BME Day in the Department of Biomedical Engineering at the GWU.  
- *2021.11*, 2021 the Artificial Intelligence for Human-Robot Interaction AAAI Fall Symposium \| [\[video\]](youtube.com/watch?v=JDMb7vyxoqM&ab_channel=ArtmedGWU)
- *2020.04*, 2020 ACM/IEEE International Conference on Human-Robot Interaction \| [\[video\]](https://www.youtube.com/watch?v=gV5mkpEhnVk&ab_channel=ACMSIGCHI)

## Poster
- *2023.04*, 2023 GW SEAS R&D Showcase at the GWU
- *2023.03*, 2023 ACM/IEEE International Conference on Human-Robot Interaction at Stockholm, SE 
- *2022.05*, 2022 GW Technology Commercialization Innovation Competition at the GWU
- *2021.07*, 2021 18th International Conference on Ubiquitous Robots (online)
- *2019.10*, 2019 Biomedical Engineering Society (BMES) Annual Meeting at Philadelphia
- *2019.10*, 2019 Research and Development Showcase at the GWU
- *2019.04*, 2019 Research Day at the GWU
- *2018.02*, 2018 GW SEAS R&D Showcase at the GWU



# üíª Experience
- *2020.06 - Present*, Graduate Research Assistant, The George Washington University
- *2018.09 - 2020/05*, Graduate Teaching Assistant, The George Washington University
  - SEAS 1001 Engineering Orientation 
  - BME 2825 Biomedical Engineering Programming II 
- *2019 01*, Mentor, 2019 George Hacks Medical Solutions Hackathon
- *2016.04 - 2016.07*, Undergraduate Group Project Advisor, Shenzhen University 