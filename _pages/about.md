---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Currently, I am a research engineer at Meta Reality Labs. My mission is to research new augmented reality (AR) input devices with a focus on human-computer interaction (HCI), input sensing, machine learning, and user interfaces. I am working with a diverse and highly interdisciplinary team of researchers and engineers and will have access to cutting-edge technology, resources, and testing facilities.

Previsouly, I am a graduate research assistant at the George Washington University. I was conferred with a Doctor of Philosophy degree by the end of the year 2023. My doctoral research is at the intersection of machine learning and human-robot interaction (HRI). From a methodological perspective, the objective of my doctoral study is to utilize machine learning algorithms together with emotional intelligence to facilitate the creation of more engaging and empathetic experiences in HRI. Throughout the duration of my doctoral research, the robotic system that I have developed has successfully attained the capability of engaging in interactive emotional communication by utilizing both verbal and non-verbal social cues. Additionally, I had the task of designing and assessing a social-emotional robot that incorporated an interactive gaming scenario. In addition, a user¬†study was conducted on the empathetic robotic companion that has been designed specifically for autistic adolescents, along with a personalized intervention program.





# üî• News
- *2024.04*: &nbsp;üíª üíª I am thrilled to start a new position as a research engineer at Meta Reality Labs!
- *2023.11*: &nbsp;üéâüéâ I have pased my PhD dissertation defense!

# üíª Experience
- *2024.04 - Present*, Research Engineer, Meta Reality Labs
- *2020.06 - 2023.12*, Graduate Research Assistant, The George Washington University
- *2018.09 - 2020.05*, Graduate Teaching Assistant, The George Washington University
  - SEAS 1001 Engineering Orientation 
  - BME 2825 Biomedical Engineering Programming II 
- *2019.01*, Mentor, 2019 George Hacks Medical Solutions Hackathon
- *2017.02 - 2018.05*, Graduate Researcher, The George Washington University 
- *2014.10 - 2016.05*, Undergraduate Researcher, Shenzhen University
  - Design of Perovskite Solar Cells 
  - Laboratory Open Fund Project of Shenzhen University
  - Shenzhen University 2015 Challenge Cup University Student Venture Contest
- *2015.07 - 2015.08*, Intern, Shenzhen JBT Smart Lighting Co., Ltd.,



# üìù Projects and Publications 

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2016</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

**Kaiming He**, Xiangyu Zhang, Shaoqing Ren, Jian Sun

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div> -->


## Human-Robot Interaction

<div class='paper-box'>
<div class='paper-box-image'><div><div class="badge">CAREER</div><img src='images/nsf_project.png' alt="sym" width="120%"></div></div>


<div class='paper-box-text' markdown="1">

*2019.03 - 2023.12*: &nbsp; **<font color='red'> As a core member, </font>** I participated in the project of [CAREER: Social Intelligence with Contextual Ambidexterity for Long-Term Human-Robot Interaction and Intervention (LT-HRI2)](http://www.chunghyukpark.com/robots-for-autistic-adolescents.html). 

- The goal of this project was to understand the fundamental principles of human interactions and behaviors and translate these mechanisms into computational modeling and algorithms for a novel assistive robotic framework. Toward this goal, we developed a socially assistive robotic framework with contextual ambidexterity that is perceptive of personal socio-emotional states, capable of learning social skills, emotionally interactive, and gender-smart for long-term human-robot interaction and intervention. Several publications related to this project include:

</div>

<!-- Publications -->
<div class='publications' markdown="1">

- [An Empathetic Social Robot with Modular Anxiety Interventions for Autistic Adolescents](https://ieeexplore.ieee.org/abstract/document/10731248), **Baijun Xie**, Chung Hyuk Park, *2024 IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)*

- [" Can You Guess My Moves? Playing Charades with a Humanoid Robot Employing Mutual Learning with Emotional Intelligence](https://dl.acm.org/doi/abs/10.1145/3568294.3580170), **Baijun Xie**, Chung Hyuk Park, *Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction*

- [Robust Multimodal Emotion Recognition from Conversation with Transformer-Based Crossmodality Fusion](https://www.mdpi.com/1424-8220/21/14/4913), **Baijun Xie**, Mariia Sidulova, Chung Hyuk Park, *Sensors*, 2021

- [Empathetic Robot With Transformer-Based Dialogue Agent](https://ieeexplore.ieee.org/abstract/document/9494669), **Baijun Xie**, Chung Hyuk Park, *2021 18th International Conference on Ubiquitous Robots (UR)*

- [Dance with a Robot: Encoder-Decoder Neural Network for Music-Dance Learning](https://dl.acm.org/doi/abs/10.1145/3371382.3378372), **Baijun Xie**, Chung Hyuk Park, *Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction*

- [Musical emotion recognition with spectral feature extraction based on a sinusoidal model with model-based and deep-learning approaches](https://www.mdpi.com/2076-3417/10/3/902), **Baijun Xie**, Jonathan C Kim, Chung Hyuk Park, *Applied Sciences*, 2020

</div>
</div>


## Social Intelligence

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/siq_ex.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Multi-Modal Correlated Network with Emotional Reasoning Knowledge for Social Intelligence Question-Answering](https://openaccess.thecvf.com/content/ICCV2023W/ASI/papers/Xie_Multi-Modal_Correlated_Network_with_Emotional_Reasoning_Knowledge_for_Social_Intelligence_ICCVW_2023_paper.pdf)

**Baijun Xie**, Chung Hyuk Park, *Proceedings of the IEEE/CVF International Conference on Computer Vision* [\[Code\]](https://github.com/Derekxbj/Social-IQ-2.0-Multimodal-with-Emotional-Cues)

- We participated in the [Social IQ 2.0 Challenge](https://cmu-multicomp-lab.github.io/social-iq-2.0/), which is designed to benchmark recent AI technologies' skills to reason about social interactions, which is referred to as Artificial Social Intelligence.
- We developed a framework named Multi-Modal Temporal Correlated Network with Emotional Social Cues (MMTC-ESC). MMTC-ESC exhibits an attention-based mechanism to model cross-modal correlations and utilizes contrastive learning for reasoning about emotional social cues.


</div>
</div>




## Medical Imaging

<div class='paper-box'>
  <div class='paper-box-image'>
    <div>
      <div class="badge">NERVE</div>
      <img src='images/nerve_project.jpg' alt="sym" width="120%">
      <div style="text-align: center;  margin-top: 20px;">
        <img src='files/nerve.gif' alt="animated gif" width="80%">
      </div>
    </div>
  </div>

<div class='paper-box-text' markdown="1">

*2020.09 - 2022.12*: &nbsp; **<font color='red'> As a core member, </font>** I participated in the project of [Real-Time Nerve Tissue Detection and Visualization](http://www.chunghyukpark.com/real-time-nerve-tissue-detection-and-visualization.html). 

- We proposed the use of birefringence images with deep learning inference to help with the detection of nerves. The study findings showed that leveraging birefringence images outperforms its RGB counterpart for nerve detection and segmentation. Additionally, introducing the cross-attention module on a multi-modality network improved the detection and segmentation of nerve structures on the birefringence images.

</div>

<!-- Publications -->
<div class='publications' markdown="1">

- [DXM‚ÄêTransFuse U-net: Dual cross-modal transformer fusion U-net for automated nerve identification](https://www.sciencedirect.com/science/article/pii/S0895611122000635), **Baijun Xie**, Gary Milam, Bo Ning, Jaepyeong Cha, Chung Hyuk Park, *Computerized Medical Imaging and Graphics*, 2022

- ``Patent`` Real Time Automated Nerve Identification System.‚Äù International Publication No. WO2023/183930, Gary Milam, **Baijun Xie**, Chung Hyuk Park

</div>
</div>


## Robotics

<div class='paper-box'>
  <div class='paper-box-image'>
    <div style="display: flex; justify-content: center; align-items: flex-start; gap: 20px;">
      <div style="text-align: center;">
        <div class="badge">ROBOT</div>
        <img src='files/platform_test.gif' alt="sym" width="80%">
      </div>
      <div style="text-align: center;">
        <img src='files/simulation.gif' alt="animated gif" width="80%">
      </div>
    </div>
  </div>

  <div class='paper-box-text' markdown="1">
    *2017.02 - 2018.05*: &nbsp; I participated in the project of "Multi-Domain Search and Rescue using Cooperative Robots" during my Master‚Äôs research.

    - I contributed to the project where we developed a collaborative system integrating aerial quadrotors and ground robots for efficient rescue operations. We focused on enabling the quadrotors to autonomously detect and track ground robots using advanced computer vision techniques, ensuring seamless communication and coordination between the two platforms.
  </div>
</div>

<!-- - [Multi-Modal Correlated Network with Emotional Reasoning Knowledge for Social Intelligence Question-Answering](https://openaccess.thecvf.com/content/ICCV2023W/ASI/html/Xie_Multi-Modal_Correlated_Network_with_Emotional_Reasoning_Knowledge_for_Social_Intelligence_ICCVW_2023_paper.html), Baijun Xie, Chung Hyuk Park, *Proceedings of the IEEE/CVF International Conference on Computer Vision* -->


## Others

- [Real-time teleoperation of magnetic force-driven microrobots with a motion model and stable haptic force feedback for micromanipulation.](https://doi.org/10.1063/10.0034396), Duygu, Yasin Cagatay, **Baijun Xie**, Xiao Zhang, Min Jun Kim, and Chung Hyuk Park, *Nanotechnology and Precision Engineering*, 2025

- [A MultiModal Social Robot Toward Personalized Emotion Interaction](https://arxiv.org/pdf/2110.05186.pdf), **Baijun Xie**, Chung Hyuk Park, *arXiv preprint*, Submitted to AAAI 2023 Fall Symposium

- [Trainable Quaternion Extended Kalman Filter with Multi-Head Attention for Dead Reckoning in Autonomous Ground Vehicles](https://www.mdpi.com/1424-8220/22/20/7701), Gary Milam, **Baijun Xie**, Runnan Liu, Xiaoheng Zhu, Juyoun Park, Gonwoo Kim, Chung Hyuk Park, *Sensors*, 2022

- Practical Numerical Methods with Python for Shallow Water Equations (SWEs) [[simulation]](https://nbviewer.org/github/Derekxbj/mae-6286/blob/1e23dc1be3c44ad70cb7840ba3bb50cd041e9aba/final_project/Final.ipynb)





# üéñ Honors and Awards
- *2023.01* 2022 Collins Distinguished Doctoral Fellowship at the GWU. 
- *2022.04* 2022 GW [Technology Commercialization Innovation Competition](https://commercialization.gwu.edu/tco-innovation-competition) ([Audience‚Äôs choice posters Prize](https://technologies.research.gwu.edu/technology/46453)). 
- *2022.01* 2021 Collins Distinguished Doctoral Fellowship at the GWU. 
- *2016.05* 2016 A Hundred Excellent Final Year Theses Prize in Shenzhen University . 
- *2015.09* 2015 Shenzhen University Challenge Cup University Student Venture Contest (2nd Prize). 
- *2014.09* Laboratory Open Fund Project of Shenzhen University (3rd Prize). 


# üìñ Educations
- *2018.09 - 2023.12*, Doctor of Philosophy, The George Washington University, Washington DC, U.S.A.
  - Related courses: Probability for Computer Science, Design & Analysis of Algorithm, Machine Learning, Pattern Recognition, Digital Image Processing, Robotics Vision and Perception, Biomedical Signal Analysis
- *2016.09 - 2018.05*, Master of Science, The George Washington University, Washington DC, U.S.A. 
  - Related courese: Analytical Methods in Engineering,  Electromechanical Control System, Robotic Systems, Mechatronics Design, Numerical Solution Tech in MAE, Applied Nonlinear Control, Applied Optimal Control & Estimation, Spacecraft Attitude Control, System Optimization, Advanced Mechanical Engineering Design
- *2012.09 - 2016.06*, Bachelor of Engineering, Shenzhen University, Guandong, China. 

# üí¨ Invited Talks
## Oral
- *2023.10*, 2023 IEEE/CVF International Conference on Computer Vision (ICCV) Workshops at Paris, France
- *2022.11*, 2022 BME Day in the Department of Biomedical Engineering at the GWU.  
- *2021.11*, 2021 the Artificial Intelligence for Human-Robot Interaction AAAI Fall Symposium \| [\[video\]](youtube.com/watch?v=JDMb7vyxoqM&ab_channel=ArtmedGWU)
- *2020.04*, 2020 ACM/IEEE International Conference on Human-Robot Interaction \| [\[video\]](https://www.youtube.com/watch?v=gV5mkpEhnVk&ab_channel=ACMSIGCHI)

## Poster
- *2023.04*, 2023 GW SEAS R&D Showcase at the GWU
- *2023.03*, 2023 ACM/IEEE International Conference on Human-Robot Interaction at Stockholm, SE 
- *2022.05*, 2022 GW Technology Commercialization Innovation Competition at the GWU
- *2021.07*, 2021 18th International Conference on Ubiquitous Robots (online)
- *2019.10*, 2019 Biomedical Engineering Society (BMES) Annual Meeting at Philadelphia
- *2019.10*, 2019 Research and Development Showcase at the GWU
- *2019.04*, 2019 Research Day at the GWU
- *2018.02*, 2018 GW SEAS R&D Showcase at the GWU

